# MistralAI

## Docs

[Agents & Conversations](https://docs.mistral.ai/docs/agents/agents_and_conversations.md): Agents, Conversations, and Entries enhance API interactions with tools, history, and flexible event representation
[Agents Function Calling](https://docs.mistral.ai/docs/agents/agents_function_calling.md): Agents use function calling to execute tools and workflows, with built-in and customizable JSON schema support
[Agents Introduction](https://docs.mistral.ai/docs/agents/agents_introduction.md): AI agents are autonomous systems powered by LLMs that plan, use tools, and execute tasks, with APIs for multimodal models, persistent state, and workflow collaboration
[Code Interpreter](https://docs.mistral.ai/docs/agents/connectors/code_interpreter.md): Code Interpreter enables secure, on-demand code execution for data analysis, graphing, and more in isolated containers
[Connectors Overview](https://docs.mistral.ai/docs/agents/connectors/connectors_overview.md): Connectors enable Agents and users to access tools like websearch, code interpreter, and more for on-demand answers
[Document Library](https://docs.mistral.ai/docs/agents/connectors/document_library.md): Document Library is a built-in RAG tool for agents to access and manage uploaded documents via Mistral Cloud
[Image Generation](https://docs.mistral.ai/docs/agents/connectors/image_generation.md): Image Generation tool enables agents to create images on demand
[Websearch](https://docs.mistral.ai/docs/agents/connectors/websearch.md): Websearch enables models to browse the web for real-time, up-to-date information or access specific URLs
[Agents Handoffs](https://docs.mistral.ai/docs/agents/handoffs.md): Agents Handoffs enable seamless task delegation and conversation handover between multiple agents in automated workflows
[MCP](https://docs.mistral.ai/docs/agents/mcp.md): MCP standardizes AI model integration with data sources for seamless, secure, and efficient contextual access
[Audio & Transcription](https://docs.mistral.ai/docs/capabilities/audio_and_transcription.md): Audio & Transcription: Models for chat and transcription with audio input support
[Batch Inference](https://docs.mistral.ai/docs/capabilities/batch_inference.md): Prepare and upload batch requests, then create a job to process them with specified models and endpoints
[Citations and References](https://docs.mistral.ai/docs/capabilities/citations_and_references.md): Citations and references enable models to ground responses with sources, enhancing accuracy in RAG and agentic applications
[Coding](https://docs.mistral.ai/docs/capabilities/coding.md): LLMs for coding: Codestral for code generation, Devstral for agentic tool use, with FIM and chat endpoints
[Annotations](https://docs.mistral.ai/docs/capabilities/document_ai/annotations.md): Mistral Document AI API enhances OCR with structured JSON annotations for bbox and document data extraction
[Basic OCR](https://docs.mistral.ai/docs/capabilities/document_ai/basic_ocr.md): Extract text and structure from PDFs with Mistral's Document AI OCR processor." (99 characters)
[Document AI](https://docs.mistral.ai/docs/capabilities/document_ai/document_ai_overview.md): Mistral Document AI offers enterprise-level OCR and structured data extraction with multilingual support and scalable workflows
[Document QnA](https://docs.mistral.ai/docs/capabilities/document_ai/document_qna.md): Document AI QnA combines OCR and LLMs to enable natural language queries for document insights and analysis
[Code Embeddings](https://docs.mistral.ai/docs/capabilities/embeddings/code_embeddings.md): Code embeddings enable powerful retrieval, clustering, and analytics for code databases and assistants
[Embeddings Overview](https://docs.mistral.ai/docs/capabilities/embeddings/embeddings_overview.md): Mistral AI's Embeddings API provides advanced vector representations for text and code, enabling NLP tasks like retrieval, clustering, and search
[Text Embeddings](https://docs.mistral.ai/docs/capabilities/embeddings/text_embeddings.md): Generate 1024-dimension text embeddings using Mistral AI's embeddings API for NLP applications
[Classifier Factory](https://docs.mistral.ai/docs/capabilities/finetuning/classifier-factory.md): Classifier Factory: Tools for moderation, intent detection, and sentiment analysis to enhance efficiency and user experience
[Fine-tuning Overview](https://docs.mistral.ai/docs/capabilities/finetuning/finetuning_overview.md): Learn about fine-tuning costs, benefits vs. prompting, and when to use each method for AI models
[Text & Vision Fine-tuning](https://docs.mistral.ai/docs/capabilities/finetuning/text-vision-finetuning.md): Fine-tune text and vision models for domain-specific tasks or unique conversational styles using your dataset
[Function calling](https://docs.mistral.ai/docs/capabilities/function-calling.md): Mistral models enable function calling to integrate external tools for enhanced application development
[Moderation](https://docs.mistral.ai/docs/capabilities/moderation.md): New moderation API with Mistral model for detecting harmful text in raw and conversational content
[Predicted outputs](https://docs.mistral.ai/docs/capabilities/predicted-outputs.md): Optimizes response time by predefining predictable content for faster, high-quality outputs." (99 characters)
[Reasoning](https://docs.mistral.ai/docs/capabilities/reasoning.md): Reasoning enhances CoT by generating logical steps before final answers, improving problem-solving depth and accuracy
[Custom Structured Output](https://docs.mistral.ai/docs/capabilities/structured-output/custom.md): Define structured JSON outputs using Pydantic models with Mistral AI for consistent, typed responses
[JSON mode](https://docs.mistral.ai/docs/capabilities/structured-output/json-mode.md): Enable JSON mode by setting `response_format` to `{\"type\": \"json_object\"}` in API requests
[Structured Output](https://docs.mistral.ai/docs/capabilities/structured-output/overview.md): Learn to generate structured outputs (JSON or custom) for LLM agents and pipelines with reliability tips
[Text and Chat Completions](https://docs.mistral.ai/docs/capabilities/text_and_chat_completions.md): Mistral models enable chat and text completions via natural language prompts, with flexible API options for streaming and async responses
[Vision](https://docs.mistral.ai/docs/capabilities/vision.md): Vision capabilities enable models to analyze images and text for multimodal insights, with support for URL and base64 image inputs
[AWS Bedrock](https://docs.mistral.ai/docs/deployment/cloud/aws.md): Deploy Mistral AI models on AWS Bedrock as fully managed, serverless endpoints." (99 characters)
[Azure AI](https://docs.mistral.ai/docs/deployment/cloud/azure.md): Deploy Mistral AI models on Azure AI with pay-as-you-go or GPU-based real-time endpoints
[IBM watsonx.ai](https://docs.mistral.ai/docs/deployment/cloud/ibm-watsonx.md): Mistral AI's Large model on IBM watsonx.ai for managed and on-premise deployments with API access setup
[Outscale](https://docs.mistral.ai/docs/deployment/cloud/outscale.md): Deploy and query Mistral AI models on Outscale via managed VMs and GPUs
[Cloud](https://docs.mistral.ai/docs/deployment/cloud/overview.md): Access Mistral AI models via Azure, AWS, Google Cloud, Snowflake, IBM, and Outscale using cloud credits
[Snowflake Cortex](https://docs.mistral.ai/docs/deployment/cloud/sfcortex.md): Access Mistral AI models on Snowflake Cortex as fully managed, serverless endpoints
[Vertex AI](https://docs.mistral.ai/docs/deployment/cloud/vertex.md): Deploy and query Mistral AI models on Google Cloud's serverless Vertex AI platform." (99 characters)
[Workspaces](https://docs.mistral.ai/docs/deployment/laplateforme/organization.md): La Plateforme workspaces enable team collaboration, access management, and shared fine-tuned models." (99 characters)
[La Plateforme](https://docs.mistral.ai/docs/deployment/laplateforme/overview.md): Mistral AI's pay-as-you-go API platform for accessing latest large language models." (99 characters)
[Pricing](https://docs.mistral.ai/docs/deployment/laplateforme/pricing.md): Check the pricing page for detailed API cost information." (99 characters)
[Rate limit and usage tiers](https://docs.mistral.ai/docs/deployment/laplateforme/tier.md): Learn about Mistral's API rate limits, usage tiers, and how to check or upgrade your workspace limits
[Deploy with Cerebrium](https://docs.mistral.ai/docs/deployment/self-deployment/cerebrium.md): Deploy AI apps effortlessly with Cerebrium's serverless GPU infrastructure, auto-scaling and pay-per-use
[Deploy with Cloudflare Workers AI](https://docs.mistral.ai/docs/deployment/self-deployment/cloudflare.md): Deploy AI models on Cloudflare's global network with Workers AI for serverless GPU-powered LLMs
[Self-deployment](https://docs.mistral.ai/docs/deployment/self-deployment/overview.md): Deploy Mistral AI models on your infrastructure using vLLM, TensorRT-LLM, TGI, or tools like SkyPilot and Cerebrium
[Deploy with SkyPilot](https://docs.mistral.ai/docs/deployment/self-deployment/skypilot.md): Deploy AI models on any cloud with SkyPilot for cost savings and high GPU availability
[Text Generation Inference](https://docs.mistral.ai/docs/deployment/self-deployment/tgi.md): TGI is a high-performance toolkit for deploying and serving open-access LLMs with features like quantization and streaming
[TensorRT](https://docs.mistral.ai/docs/deployment/self-deployment/trt.md): Guide to building and deploying TensorRT-LLM engines for Mistral and Mixtral models using Triton Inference Server
[vLLM](https://docs.mistral.ai/docs/deployment/self-deployment/vllm.md): vLLM is an open-source LLM inference engine optimized for deploying Mistral models on-premise
[SDK Clients](https://docs.mistral.ai/docs/getting-started/clients.md): SDK clients for Python, Typescript, and community-supported languages
[Bienvenue to Mistral AI Documentation](https://docs.mistral.ai/docs/getting-started/docs_introduction.md): Mistral AI offers open-source and commercial LLMs for developers, with premier models like Mistral Medium and Codestral
[Glossary](https://docs.mistral.ai/docs/getting-started/glossary.md): Glossary of key terms related to large language models, text generation, and AI concepts
[Model customization](https://docs.mistral.ai/docs/getting-started/model_customization.md): Guide to building applications with custom LLMs for deployment, emphasizing iterative development and user feedback
[Models Benchmarks](https://docs.mistral.ai/docs/getting-started/models/benchmark.md): Standardized tests evaluating LLM performance, including Mistral's top-tier reasoning and multilingual benchmarks
[Model selection](https://docs.mistral.ai/docs/getting-started/models/model_selection.md): Guide to selecting Mistral models based on performance, cost, and use-case complexity
[Models Overview](https://docs.mistral.ai/docs/getting-started/models/overview.md): Mistral offers open and premier models, including multimodal and reasoning options with API access
[Model weights](https://docs.mistral.ai/docs/getting-started/models/weights.md): Open-source pre-trained and instruction-tuned models with varying licenses; commercial options available
[Quickstart](https://docs.mistral.ai/docs/getting-started/quickstart.md): Quickstart guide for setting up a Mistral AI account, managing billing, and generating API keys
[Basic RAG](https://docs.mistral.ai/docs/guides/basic-RAG.md): RAG combines LLMs with retrieval systems to generate answers using external knowledge." (99 characters)
[Ambassador](https://docs.mistral.ai/docs/guides/contribute/ambassador.md): Join Mistral AI's Ambassador Program to advocate for AI models, share expertise, and support the community. Apply by July 1, 2025
[Contribute](https://docs.mistral.ai/docs/guides/contribute/overview.md): Learn how to contribute to Mistral AI through docs, code, or the Ambassador Program
[Evaluation](https://docs.mistral.ai/docs/guides/evaluation.md): Guide to evaluating LLMs for specific use cases with metrics, LLM, and human-based methods." (99 characters)
[Fine-tuning](https://docs.mistral.ai/docs/guides/finetuning.md): Fine-tuning models incurs a $2 monthly storage fee; see pricing for details
[ 01 Intro Basics](https://docs.mistral.ai/docs/guides/finetuning_sections/_01_intro_basics.md): Learn the basics of fine-tuning LLMs to optimize performance for specific tasks using Mistral AI's tools
[ 02 Prepare Dataset](https://docs.mistral.ai/docs/guides/finetuning_sections/_02_prepare_dataset.md): Prepare training data for fine-tuning models with specific use cases and examples
[download the validation and reformat script](https://docs.mistral.ai/docs/guides/finetuning_sections/_03_e2e_examples.md): Download the reformat_data.py script to validate and reformat Mistral API fine-tuning datasets
[get data from hugging face](https://docs.mistral.ai/docs/guides/finetuning_sections/_04_faq.md): Learn how to fetch, validate, and format data from Hugging Face for Mistral models
[Observability](https://docs.mistral.ai/docs/guides/observability.md): Observability ensures visibility, debugging, and continuous improvement for LLM systems in production." (99 characters)
[Other resources](https://docs.mistral.ai/docs/guides/other-resources.md): Explore Mistral AI Cookbook for code examples, community contributions, and third-party tool integrations
[Prefix](https://docs.mistral.ai/docs/guides/prefix.md): Prefixes enhance instruction adherence and response control for various use cases." (99 characters)
[Prompting capabilities](https://docs.mistral.ai/docs/guides/prompting-capabilities.md): Learn how to craft effective prompts for classification, summarization, personalization, and evaluation with Mistral models
[Sampling](https://docs.mistral.ai/docs/guides/sampling.md): Learn how to adjust LLM sampling parameters like Temperature, Top P, and penalties for better output control
[Tokenization](https://docs.mistral.ai/docs/guides/tokenization.md): Tokenization breaks text into subword units for LLM processing, with Mistral AI's open-source tools and Python integration