import { Tabs, TabItem } from '@/components/common/multi-codeblock';
import { SectionTab } from '@/components/layout/section-tab';

Text embeddings can be used as input features in machine learning models, such as classification and clustering. In this example, we use a classification model to predict the disease labels from the embeddings of disease description text from the previous **Batching example**.

<Tabs groupId="code">
    <TabItem value="python" label="python" default>

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Create a train / test split
train_x, test_x, train_y, test_y = train_test_split(
    df["embeddings"], df["label"], test_size=0.2
)

# Normalize features
scaler = StandardScaler()
train_x = scaler.fit_transform(train_x.to_list())
test_x = scaler.transform(test_x.to_list())

# Train a classifier and compute the test accuracy
# For a real problem, C should be properly cross validated and the confusion matrix analyzed
clf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(
    train_x, train_y.to_list()
)
# you can also try the sag algorithm:
# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)

print(f"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%")
```

    </TabItem>
    <TabItem value="output" label="output">

```
Precision: 98.75%
```

    </TabItem>
</Tabs>

After we trained the classifier with our embeddings data, we can try classify other text:

<Tabs groupId="code">
    <TabItem value="python" label="python" default>

```python
# Classify a single example
text = "I've been experiencing frequent headaches and vision problems."
clf.predict([get_text_embedding([text])])
```

    </TabItem>
    <TabItem value="output" label="output">

```text
'Migraine'
```

    </TabItem>
</Tabs>

<SectionTab as="h3" variant="secondary" sectionId="comparison-with-fastText-embeddings">Comparison with fastText embeddings</SectionTab>

Additionally, let's take a look at the performance using fastText embeddings in this classification task. It appears that the classification model achieves better performance with Mistral AI Embeddings model as compared to using fastText embeddings.

<Tabs groupId="code">
    <TabItem value="python" label="python" default>
  
```python
# Create a train / test split
train_x, test_x, train_y, test_y = train_test_split(
    df["fasttext_embeddings"], df["label"], test_size=0.2
)

# Normalize features

scaler = StandardScaler()
train_x = scaler.fit_transform(train_x.to_list())
test_x = scaler.transform(test_x.to_list())

# Train a classifier and compute the test accuracy

# For a real problem, C should be properly cross validated and the confusion matrix analyzed

clf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(
train_x, train_y.to_list()
)

# you can also try the sag algorithm:

# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)

print(f"Precision: {100\*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%")

```

    </TabItem>
    <TabItem value="output" label="output">

```txt
Precision: 86.25%
````

    </TabItem>
</Tabs>

Our method using Mistral AI Embeddings model achieves a precision of **98.75%**, while the fastText embeddings method achieves a precision of 86.25%.