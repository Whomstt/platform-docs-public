import { Tabs, TabItem } from '@/components/common/multi-codeblock';

Start the inference server to deploy your model, e.g., for Mistral Small:
```bash
vllm serve mistralai/Mistral-Small-Instruct-2409 \
    --tokenizer_mode mistral \
    --config_format mistral \
    --load_format mistral
```
You can now run inference requests with text input:
<Tabs>
    <TabItem value="vllm-infer-small-curl" label="cURL">
        ```bash
        curl --location 'http://localhost:8000/v1/chat/completions' \
            --header 'Content-Type: application/json' \
            --header 'Authorization: Bearer token' \
            --data '{
                "model": "mistralai/Mistral-Small-Instruct-2409",
                "messages": [
                    {
                        "role": "user",
                        "content": "Who is the best French painter? Answer in one short sentence."
                    }
                ]
            }'
        ```
    </TabItem>
    <TabItem value="vllm-infer-small-python" label="Python">
        ```python
        import httpx
        url = 'http://localhost:8000/v1/chat/completions'
        headers = {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer token'
        }
        data = {
            "model": "mistralai/Mistral-Small-Instruct-2409",
            "messages": [
                {
                    "role": "user",
                    "content": "Who is the best French painter? Answer in one short sentence."
                }
            ]
        }
        response = httpx.post(url, headers=headers, json=data)
        print(response.json())
        ```
    </TabItem>
</Tabs>