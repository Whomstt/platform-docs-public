---
id: platform_overview
title: AI Studio
slug: overview
---

import { SectionTab } from '@/components/layout/section-tab';

[platform_url]: https://console.mistral.ai/
[deployment_img]: /img/deployment.png
[deployment_url]: https://console.mistral.ai/

import { Tabs, TabItem } from '@/components/common/multi-codeblock';

# AI Studio

Mistral AI currently provides three types of access to Large Language Models and other services: 
- **AI Studio** previously "La Plateforme": We provide API endpoints through [AI Studio][platform_url] providing pay-as-you-go access to our latest models, manage Workspaces and Usage, as well as diverse other features.
- **Third Party Cloud**: You can access Mistral AI models via your preferred [cloud platforms](/deployment/cloud/overview/).
- **Self-Deployment**: You can self-deploy our open-weights models on your own on-premise infrastructure. Our open weights models are available under the [Apache 2.0](https://github.com/apache/.github/blob/main/LICENSE) License, available on [Hugging Face](https://huggingface.co/mistralai) or directly from [the documentation](/getting-started/models/weights).
    - **Self-Deploy with Enterprise Support**: You can also self-deploy our models, both open and frontier, with enterprise support through our [partners](/deployment/self-deployment/partners).

<SectionTab as="h1" sectionId="api-access-with-ai-studio">API Access with AI Studio</SectionTab>

You will need to activate payments on your account to enable your API keys in the [AI Studio][platform_url]. Check out the [Quickstart](/getting-started/quickstart/) guide to get started with your first Mistral API request. 

Explore diverse capabilities of our models:
- [Completion](/capabilities/completion)
- [Embeddings](/capabilities/embeddings/overview)
- [Function calling](/capabilities/function_calling)
- [JSON mode](/capabilities/structured-output/json_mode)
- [Guardrailing](/capabilities/guardrailing)
- Much more...

<SectionTab as="h1" sectionId="cloud-based-deployments">Cloud-based deployments</SectionTab>

For a comprehensive list of options to deploy and consume Mistral AI models on the cloud, head on to the **[cloud deployment section](/deployment/cloud/overview)**.

<SectionTab as="h1" sectionId="raw-model-weights">Raw model weights</SectionTab>

Raw model weights can be used in several ways: 
- For self-deployment, on cloud or on premise, using either [TensorRT-LLM](/deployment/self-deployment/trt) or [vLLM](/deployment/self-deployment/vllm), head on to **[Deployment](/deployment/self-deployment/skypilot)**
- For research, head-on to our [reference implementation repository](https://github.com/mistralai/mistral-src),
- For local deployment on consumer grade hardware, check out the [llama.cpp](https://github.com/ggerganov/llama.cpp) project or [Ollama](https://ollama.ai/).
