---
id: platform_overview
title: AI Studio
slug: overview
---

import { SectionTab } from '@/components/layout/section-tab';

[platform_url]: https://console.mistral.ai/
[deployment_img]: /img/deployment.png
[deployment_url]: https://console.mistral.ai/

import { Tabs, TabItem } from '@/components/common/multi-codeblock';

# AI Studio

**Mistral AI Studio** is a platform where you can access and manage models, usage, APIs, organizations, workspaces, and a variety of other features.  
We offer flexible access to our models through a range of options, services, and customizable solutions-including playgrounds, fine-tuning, and more-to meet your specific needs.

<a href="https://console.mistral.ai/home" target="_blank">
    <img src="/img/aistudio.png" width="80%" className='mx-auto' />
</a>

Mistral AI currently provides three general types of access to Large Language Models and other services: 
- **AI Studio** previously "La Plateforme": We provide API endpoints through [AI Studio][platform_url] providing pay-as-you-go access to our latest models, manage Workspaces and Usage, as well as diverse other features.
- **Third Party Cloud**: You can access Mistral AI models via your preferred [cloud platforms](/deployment/cloud/overview/).
- **Self-Deployment**: You can self-deploy our open-weights models on your own on-premise infrastructure. Our open weights models are available under the [Apache 2.0](https://github.com/apache/.github/blob/main/LICENSE) License, available on [Hugging Face](https://huggingface.co/mistralai), or through our [external partners](/deployment/self-deployment/partners).
    - **Self-Deploy with Enterprise Support**: You can also self-deploy our models, both open and frontier, with enterprise support. Reach out to us [here](https://mistral.ai/contact) if youâ€™re interested!

<SectionTab as="h1" sectionId="api-access-with-ai-studio">API Access with AI Studio</SectionTab>

You will need to activate payments on your account to enable your API keys in the [AI Studio][platform_url]. Check out the [Quickstart](/getting-started/quickstart/) guide to get started with your first Mistral API request. 

Explore diverse capabilities of our models:
- [Completion](/capabilities/completion)
- [Embeddings](/capabilities/embeddings/overview)
- [Function calling](/capabilities/function_calling)
- [JSON mode](/capabilities/structured-output/json_mode)
- [Guardrailing](/capabilities/guardrailing)
- Much more...

<SectionTab as="h1" sectionId="cloud-based-deployments">Cloud-based deployments</SectionTab>

For a comprehensive list of options to deploy and consume Mistral AI models on the cloud, head on to the **[cloud deployment section](/deployment/cloud/overview)**.

<SectionTab as="h1" sectionId="raw-model-weights">Raw model weights</SectionTab>

Raw model weights can be used in several ways: 
- For self-deployment, on cloud or on premise, using either [TensorRT-LLM](/deployment/self-deployment/trt) or [vLLM](/deployment/self-deployment/vllm), head on to **[Deployment](/deployment/self-deployment/skypilot)**
- For research, head-on to our [reference implementation repository](https://github.com/mistralai/mistral-src),
- For local deployment on consumer grade hardware, check out the [llama.cpp](https://github.com/ggerganov/llama.cpp) project or [Ollama](https://ollama.ai/).
